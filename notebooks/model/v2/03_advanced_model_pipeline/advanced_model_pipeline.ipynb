{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c42107d9",
   "metadata": {},
   "source": [
    "## 3. Advanced Model Pipeline: Multi-Model Comparison & Ensemble Learning\n",
    "\n",
    "**Strategy**:\n",
    "1. **Multiple Base Models**: Test Ridge, Random Forest, Gradient Boosting, and SVR\n",
    "2. **Ensemble Methods**: Voting Regressor and Stacking Regressor\n",
    "3. **Hyperparameter Tuning**: GridSearchCV with cross-validation\n",
    "4. **Bias-Variance Analysis**: Learning curves to diagnose underfitting/overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48856db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional Imports for Ensemble Methods\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import VotingRegressor, StackingRegressor, AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Additional Feature: Avg Room Size (Proxy for spaciousness)\n",
    "df['avg_room_size'] = df['size'] / df['room_count']\n",
    "\n",
    "# Target Transformation\n",
    "df['log_price'] = np.log1p(df['price'])\n",
    "\n",
    "# Define Features\n",
    "features = ['city','region' ,'size', 'room_count', 'bathroom_count', 'avg_room_size']\n",
    "X = df[features]\n",
    "y = df['log_price']\n",
    "\n",
    "# Preprocessing\n",
    "categorical_cols = ['city','region']\n",
    "numeric_cols = ['size', 'room_count', 'bathroom_count', 'avg_room_size']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', RobustScaler(), numeric_cols),\n",
    "        ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), categorical_cols)\n",
    "    ])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f\"Training Set: {X_train.shape[0]} samples | Test Set: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1ece38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3.1 Define Base Models with Parameter Grids ---\n",
    "\n",
    "base_models = {\n",
    "    'Ridge': {\n",
    "        'model': Ridge(),\n",
    "        'params': {\n",
    "            'model__alpha': [0.1, 1.0, 10.0, 100.0]\n",
    "        }\n",
    "    },\n",
    "    'ElasticNet': {\n",
    "        'model': ElasticNet(max_iter=5000),\n",
    "        'params': {\n",
    "            'model__alpha': [0.1, 0.5, 1.0],\n",
    "            'model__l1_ratio': [0.2, 0.5, 0.8]\n",
    "        }\n",
    "    },\n",
    "    'RandomForest': {\n",
    "        'model': RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "        'params': {\n",
    "            'model__n_estimators': [100, 200],\n",
    "            'model__max_depth': [10, 20, None],\n",
    "            'model__min_samples_split': [2, 5]\n",
    "        }\n",
    "    },\n",
    "    'GradientBoosting': {\n",
    "        'model': GradientBoostingRegressor(random_state=42),\n",
    "        'params': {\n",
    "            'model__n_estimators': [100, 200],\n",
    "            'model__learning_rate': [0.05, 0.1],\n",
    "            'model__max_depth': [3, 5, 7]\n",
    "        }\n",
    "    },\n",
    "    'AdaBoost': {\n",
    "        'model': AdaBoostRegressor(random_state=42),\n",
    "        'params': {\n",
    "            'model__n_estimators': [50, 100, 150],\n",
    "            'model__learning_rate': [0.05, 0.1, 0.5]\n",
    "        }\n",
    "    },\n",
    "    'SVR': {\n",
    "        'model': SVR(),\n",
    "        'params': {\n",
    "            'model__C': [0.1, 1, 10],\n",
    "            'model__kernel': ['rbf', 'linear'],\n",
    "            'model__epsilon': [0.1, 0.2]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"Testing {len(base_models)} base models with hyperparameter tuning...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fde6d7",
   "metadata": {},
   "source": [
    "### 3.2 Multi-Model Training & Cross-Validation Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b64754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Multi-Model Training with GridSearchCV ---\n",
    "\n",
    "model_results = {}\n",
    "best_estimators = {}\n",
    "\n",
    "for name, config in base_models.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training: {name}\")\n",
    "    print('='*50)\n",
    "    \n",
    "    # Create pipeline for this model\n",
    "    pipeline = Pipeline([\n",
    "        ('prep', preprocessor),\n",
    "        ('model', config['model'])\n",
    "    ])\n",
    "    \n",
    "    # GridSearchCV with 5-fold CV\n",
    "    grid = GridSearchCV(\n",
    "        pipeline, \n",
    "        config['params'], \n",
    "        cv=5, \n",
    "        scoring='r2',\n",
    "        n_jobs=-1,\n",
    "        return_train_score=True\n",
    "    )\n",
    "    \n",
    "    grid.fit(X_train, y_train)\n",
    "    \n",
    "    # Store best estimator\n",
    "    best_estimators[name] = grid.best_estimator_\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    y_pred = grid.predict(X_test)\n",
    "    y_pred_real = np.expm1(y_pred)\n",
    "    y_test_real = np.expm1(y_test)\n",
    "    \n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test_real, y_pred_real)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_real, y_pred_real))\n",
    "    \n",
    "    # Bias-Variance indicators from CV\n",
    "    cv_train_mean = grid.cv_results_['mean_train_score'][grid.best_index_]\n",
    "    cv_test_mean = grid.cv_results_['mean_test_score'][grid.best_index_]\n",
    "    cv_test_std = grid.cv_results_['std_test_score'][grid.best_index_]\n",
    "    \n",
    "    model_results[name] = {\n",
    "        'R2_Test': r2,\n",
    "        'MAE': mae,\n",
    "        'RMSE': rmse,\n",
    "        'CV_Train_R2': cv_train_mean,\n",
    "        'CV_Test_R2': cv_test_mean,\n",
    "        'CV_Std': cv_test_std,\n",
    "        'Bias_Indicator': 1 - cv_train_mean,  # High = underfitting\n",
    "        'Variance_Indicator': cv_train_mean - cv_test_mean,  # High = overfitting\n",
    "        'Best_Params': grid.best_params_\n",
    "    }\n",
    "    \n",
    "    print(f\"Best Params: {grid.best_params_}\")\n",
    "    print(f\"Test R¬≤: {r2:.4f} | MAE: {mae:,.0f} TND | RMSE: {rmse:,.0f} TND\")\n",
    "    print(f\"CV Train R¬≤: {cv_train_mean:.4f} | CV Test R¬≤: {cv_test_mean:.4f} ¬± {cv_test_std:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"All base models trained successfully!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714a5a35",
   "metadata": {},
   "source": [
    "### 3.3 Model Comparison: Results & Bias-Variance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bb181c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Comparison Table ---\n",
    "comparison_df = pd.DataFrame(model_results).T\n",
    "comparison_df = comparison_df[['R2_Test', 'MAE', 'RMSE', 'CV_Train_R2', 'CV_Test_R2', 'CV_Std', 'Bias_Indicator', 'Variance_Indicator']]\n",
    "\n",
    "# Ensure numeric types\n",
    "numeric_cols_df = ['R2_Test', 'MAE', 'RMSE', 'CV_Train_R2', 'CV_Test_R2', 'CV_Std', 'Bias_Indicator', 'Variance_Indicator']\n",
    "for col in numeric_cols_df:\n",
    "    comparison_df[col] = pd.to_numeric(comparison_df[col], errors='coerce')\n",
    "\n",
    "comparison_df = comparison_df.round(4)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BASE MODEL COMPARISON (Sorted by Test R¬≤)\")\n",
    "print(\"=\"*70)\n",
    "print(comparison_df.sort_values('R2_Test', ascending=False).to_string())\n",
    "\n",
    "# --- Visualization ---\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: R¬≤ Comparison\n",
    "ax1 = axes[0, 0]\n",
    "models_sorted = comparison_df.sort_values('R2_Test', ascending=True).index\n",
    "colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(models_sorted)))\n",
    "ax1.barh(models_sorted, comparison_df.loc[models_sorted, 'R2_Test'], color=colors)\n",
    "ax1.set_xlabel('R¬≤ Score')\n",
    "ax1.set_title('Test R¬≤ by Model')\n",
    "ax1.axvline(x=comparison_df['R2_Test'].max(), color='red', linestyle='--', alpha=0.7, label='Best')\n",
    "\n",
    "# Plot 2: MAE Comparison  \n",
    "ax2 = axes[0, 1]\n",
    "ax2.barh(models_sorted, comparison_df.loc[models_sorted, 'MAE'], color=colors)\n",
    "ax2.set_xlabel('MAE (TND)')\n",
    "ax2.set_title('Mean Absolute Error by Model')\n",
    "ax2.axvline(x=comparison_df['MAE'].min(), color='green', linestyle='--', alpha=0.7, label='Best')\n",
    "\n",
    "# Plot 3: Bias-Variance Trade-off\n",
    "ax3 = axes[1, 0]\n",
    "for i, model in enumerate(comparison_df.index):\n",
    "    ax3.scatter(comparison_df.loc[model, 'Bias_Indicator'], \n",
    "                comparison_df.loc[model, 'Variance_Indicator'],\n",
    "                s=150, label=model)\n",
    "ax3.set_xlabel('Bias Indicator (1 - Train R¬≤) ‚Üí High = Underfitting')\n",
    "ax3.set_ylabel('Variance Indicator (Train R¬≤ - CV Test R¬≤) ‚Üí High = Overfitting')\n",
    "ax3.set_title('Bias-Variance Trade-off Analysis')\n",
    "ax3.legend(loc='upper right', fontsize=8)\n",
    "ax3.axhline(y=0.05, color='red', linestyle='--', alpha=0.5, label='Overfitting Threshold')\n",
    "ax3.axvline(x=0.2, color='orange', linestyle='--', alpha=0.5, label='Underfitting Threshold')\n",
    "\n",
    "# Plot 4: CV Score with Std\n",
    "ax4 = axes[1, 1]\n",
    "x_pos = np.arange(len(comparison_df))\n",
    "ax4.bar(x_pos, comparison_df['CV_Test_R2'], yerr=comparison_df['CV_Std'], \n",
    "        capsize=5, color=colors, alpha=0.8)\n",
    "ax4.set_xticks(x_pos)\n",
    "ax4.set_xticklabels(comparison_df.index, rotation=45, ha='right')\n",
    "ax4.set_ylabel('CV Test R¬≤ (¬± Std)')\n",
    "ax4.set_title('Cross-Validation Stability')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify best base model\n",
    "best_base_name = comparison_df['R2_Test'].idxmax()\n",
    "print(f\"\\nüèÜ Best Base Model: {best_base_name} (R¬≤ = {comparison_df.loc[best_base_name, 'R2_Test']:.4f})\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
