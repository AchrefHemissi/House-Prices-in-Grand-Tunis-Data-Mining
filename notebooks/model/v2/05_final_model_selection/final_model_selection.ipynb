{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed78e023",
   "metadata": {},
   "source": [
    "## 5. Final Model Selection & Comprehensive Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e3ecca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Final Comparison: All Models Including Ensembles ---\n",
    "final_comparison = pd.DataFrame(model_results).T[['R2_Test', 'MAE', 'RMSE']].dropna(subset=['R2_Test'])\n",
    "final_comparison = final_comparison.sort_values('R2_Test', ascending=False)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"FINAL MODEL RANKING (All Models)\")\n",
    "print(\"=\"*70)\n",
    "print(final_comparison.round(4).to_string())\n",
    "\n",
    "# Select the best model\n",
    "best_model_name = final_comparison['R2_Test'].idxmax()\n",
    "best_model = best_estimators[best_model_name]\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"ðŸ† CHAMPION MODEL: {best_model_name}\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"   RÂ² Score: {final_comparison.loc[best_model_name, 'R2_Test']:.4f}\")\n",
    "print(f\"   MAE: {final_comparison.loc[best_model_name, 'MAE']:,.0f} TND\")\n",
    "print(f\"   RMSE: {final_comparison.loc[best_model_name, 'RMSE']:,.0f} TND\")\n",
    "\n",
    "# Visualization: Final Comparison Bar Chart\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "colors = ['gold' if name == best_model_name else 'steelblue' for name in final_comparison.index]\n",
    "bars = ax.barh(final_comparison.index, final_comparison['R2_Test'], color=colors)\n",
    "ax.set_xlabel('RÂ² Score', fontsize=12)\n",
    "ax.set_title('Final Model Comparison: RÂ² Score (Higher is Better)', fontsize=14)\n",
    "ax.axvline(x=final_comparison['R2_Test'].max(), color='red', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars, final_comparison['R2_Test']):\n",
    "    ax.text(bar.get_width() + 0.005, bar.get_y() + bar.get_height()/2, \n",
    "            f'{val:.4f}', va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a7344a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Residual Analysis for Best Model ---\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "y_test_real = np.expm1(y_test)\n",
    "y_pred_real = np.expm1(y_pred_best)\n",
    "residuals = y_test_real - y_pred_real\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Plot 1: Actual vs Predicted\n",
    "ax1 = axes[0]\n",
    "ax1.scatter(y_test_real, y_pred_real, alpha=0.5, c='steelblue')\n",
    "max_val = max(y_test_real.max(), y_pred_real.max())\n",
    "ax1.plot([0, max_val], [0, max_val], 'r--', label='Perfect Prediction')\n",
    "ax1.set_xlabel('Actual Price (TND)')\n",
    "ax1.set_ylabel('Predicted Price (TND)')\n",
    "ax1.set_title(f'Actual vs Predicted: {best_model_name}')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot 2: Residual Distribution\n",
    "ax2 = axes[1]\n",
    "ax2.hist(residuals, bins=30, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "ax2.axvline(x=0, color='red', linestyle='--', label='Zero Error')\n",
    "ax2.axvline(x=residuals.mean(), color='orange', linestyle='-', label=f'Mean: {residuals.mean():,.0f}')\n",
    "ax2.set_xlabel('Residual (TND)')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.set_title('Residual Distribution')\n",
    "ax2.legend()\n",
    "\n",
    "# Plot 3: Residuals vs Predicted (Heteroscedasticity Check)\n",
    "ax3 = axes[2]\n",
    "ax3.scatter(y_pred_real, residuals, alpha=0.5, c='steelblue')\n",
    "ax3.axhline(y=0, color='red', linestyle='--')\n",
    "ax3.set_xlabel('Predicted Price (TND)')\n",
    "ax3.set_ylabel('Residual (TND)')\n",
    "ax3.set_title('Residuals vs Predicted (Checking Heteroscedasticity)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(f'Residual Analysis: {best_model_name}', y=1.02, fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nResidual Statistics:\")\n",
    "print(f\"  Mean Error: {residuals.mean():,.0f} TND\")\n",
    "print(f\"  Std Error: {residuals.std():,.0f} TND\")\n",
    "print(f\"  Median Error: {residuals.median():,.0f} TND\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d242b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5.1 Bias-Variance Analysis Summary (Dynamic) ---\n",
    "from IPython.display import display, Markdown\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def generate_bias_variance_summary(model_results, best_estimators, X_train, y_train):\n",
    "    \"\"\"Generate dynamic bias-variance analysis from model results\n",
    "    \n",
    "    Bias-Variance decomposition (using CV scores as proxies):\n",
    "    - Bias Indicator = max(0, 1 - CV_Train_RÂ²) \n",
    "      â†’ High value means model can't fit training data well (underfitting)\n",
    "    - Variance Indicator = |CV_Train_RÂ² - CV_Test_RÂ²| (absolute value, always non-negative)\n",
    "      â†’ High value means unstable performance between train/test\n",
    "    \"\"\"\n",
    "    \n",
    "    # Include base models AND ensemble models\n",
    "    analysis_models = ['RandomForest', 'GradientBoosting', 'SVR', 'Ridge', 'ElasticNet', \n",
    "                       'Voting_Ensemble', 'Stacking_Ensemble']\n",
    "    model_analysis = []\n",
    "    \n",
    "    for model_name in analysis_models:\n",
    "        if model_name in model_results:\n",
    "            # Use the pre-computed values from GridSearchCV if available\n",
    "            cv_train_r2 = model_results[model_name].get('CV_Train_R2', None)\n",
    "            cv_test_r2 = model_results[model_name].get('CV_Test_R2', None)\n",
    "            \n",
    "            # For ensemble models, compute CV scores if not available\n",
    "            if (cv_train_r2 is None or (isinstance(cv_train_r2, float) and np.isnan(cv_train_r2))) and model_name in best_estimators:\n",
    "                print(f\"Computing CV scores for {model_name}...\")\n",
    "                model = best_estimators[model_name]\n",
    "                \n",
    "                # Compute cross-validation scores\n",
    "                cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')\n",
    "                cv_test_r2 = cv_scores.mean()\n",
    "                \n",
    "                # For ensemble train score, fit and predict on training data\n",
    "                model.fit(X_train, y_train)\n",
    "                y_train_pred = model.predict(X_train)\n",
    "                cv_train_r2 = r2_score(y_train, y_train_pred)\n",
    "                \n",
    "                # Update model_results for future reference\n",
    "                model_results[model_name]['CV_Train_R2'] = cv_train_r2\n",
    "                model_results[model_name]['CV_Test_R2'] = cv_test_r2\n",
    "                model_results[model_name]['CV_Std'] = cv_scores.std()\n",
    "            \n",
    "            if cv_train_r2 is not None and cv_test_r2 is not None:\n",
    "                # Skip if NaN\n",
    "                if isinstance(cv_train_r2, float) and np.isnan(cv_train_r2):\n",
    "                    continue\n",
    "                if isinstance(cv_test_r2, float) and np.isnan(cv_test_r2):\n",
    "                    continue\n",
    "                \n",
    "                # Bias: How well the model fits training data (1 - train score)\n",
    "                # Use max(0, ...) to ensure non-negative (train RÂ² can exceed 1 in rare cases)\n",
    "                bias_indicator = max(0, 1 - cv_train_r2)\n",
    "                \n",
    "                # Variance: Gap between train and test CV scores\n",
    "                # Use absolute value to ensure non-negative \n",
    "                # (test can exceed train due to CV variance - this indicates good generalization)\n",
    "                raw_gap = cv_train_r2 - cv_test_r2\n",
    "                variance_indicator = abs(raw_gap)\n",
    "                \n",
    "                # Diagnose the model\n",
    "                if raw_gap > 0.08:\n",
    "                    diagnosis = \"**Overfitting** (train >> test)\"\n",
    "                elif raw_gap < -0.03:\n",
    "                    diagnosis = \"Good generalization (test > train)\"\n",
    "                elif variance_indicator < 0.03 and bias_indicator > 0.15:\n",
    "                    diagnosis = \"**Underfitting** (high bias)\"\n",
    "                elif variance_indicator < 0.05 and bias_indicator < 0.15:\n",
    "                    diagnosis = \"**Best balance**\"\n",
    "                else:\n",
    "                    diagnosis = \"Good fit\"\n",
    "                \n",
    "                model_analysis.append({\n",
    "                    'Model': model_name,\n",
    "                    'CV_Train_R2': cv_train_r2,\n",
    "                    'CV_Test_R2': cv_test_r2,\n",
    "                    'Bias': bias_indicator,\n",
    "                    'Variance': variance_indicator,\n",
    "                    'Raw_Gap': raw_gap,\n",
    "                    'Diagnosis': diagnosis,\n",
    "                    'Is_Ensemble': model_name in ['Voting_Ensemble', 'Stacking_Ensemble']\n",
    "                })\n",
    "    \n",
    "    # Build markdown table - separate base models and ensembles\n",
    "    md_content = \"### 5.1 Bias-Variance Analysis Summary\\n\\n\"\n",
    "    md_content += \"**Understanding the metrics:**\\n\"\n",
    "    md_content += \"- **Bias** = max(0, 1 - CV_Train_RÂ²) â†’ high = underfitting\\n\"\n",
    "    md_content += \"- **Variance** = |CV_Train_RÂ² - CV_Test_RÂ²| â†’ high = unstable fit\\n\"\n",
    "    md_content += \"- **Gap** = CV_Train_RÂ² - CV_Test_RÂ² â†’ positive = overfitting, negative = good generalization\\n\\n\"\n",
    "    \n",
    "    # Base models table\n",
    "    base_models = [m for m in model_analysis if not m['Is_Ensemble']]\n",
    "    if base_models:\n",
    "        md_content += \"#### Base Models\\n\"\n",
    "        md_content += \"| Model | CV Train RÂ² | CV Test RÂ² | Bias | Variance | Gap | Diagnosis |\\n\"\n",
    "        md_content += \"|-------|-------------|------------|------|----------|-----|----------|\\n\"\n",
    "        for model in base_models:\n",
    "            gap_str = f\"{model['Raw_Gap']:+.3f}\"\n",
    "            md_content += f\"| {model['Model']} | {model['CV_Train_R2']:.4f} | {model['CV_Test_R2']:.4f} | {model['Bias']:.3f} | {model['Variance']:.3f} | {gap_str} | {model['Diagnosis']} |\\n\"\n",
    "    \n",
    "    # Ensemble models table\n",
    "    ensemble_models = [m for m in model_analysis if m['Is_Ensemble']]\n",
    "    if ensemble_models:\n",
    "        md_content += \"\\n#### Ensemble Models\\n\"\n",
    "        md_content += \"| Model | CV Train RÂ² | CV Test RÂ² | Bias | Variance | Gap | Diagnosis |\\n\"\n",
    "        md_content += \"|-------|-------------|------------|------|----------|-----|----------|\\n\"\n",
    "        for model in ensemble_models:\n",
    "            gap_str = f\"{model['Raw_Gap']:+.3f}\"\n",
    "            md_content += f\"| {model['Model']} | {model['CV_Train_R2']:.4f} | {model['CV_Test_R2']:.4f} | {model['Bias']:.3f} | {model['Variance']:.3f} | {gap_str} | {model['Diagnosis']} |\\n\"\n",
    "    \n",
    "    # Find best balanced model (lowest combined bias + variance)\n",
    "    if model_analysis:\n",
    "        sorted_models = sorted(model_analysis, key=lambda x: x['Bias'] + x['Variance'])\n",
    "        best_model = sorted_models[0]['Model']\n",
    "        best_base = sorted([m for m in model_analysis if not m['Is_Ensemble']], \n",
    "                          key=lambda x: x['Bias'] + x['Variance'])[0]['Model'] if base_models else None\n",
    "    else:\n",
    "        best_model = \"Unknown\"\n",
    "        best_base = None\n",
    "    \n",
    "    md_content += \"\\n**Key Observations:**\\n\"\n",
    "    \n",
    "    # Compare ensemble vs base models\n",
    "    if ensemble_models and base_models:\n",
    "        best_ensemble = min(ensemble_models, key=lambda x: x['Variance'])\n",
    "        best_base_model = min(base_models, key=lambda x: x['Variance'])\n",
    "        \n",
    "        if best_ensemble['Variance'] < best_base_model['Variance']:\n",
    "            md_content += f\"1. **{best_ensemble['Model']}** has lower variance ({best_ensemble['Variance']:.3f}) than the best base model ({best_base_model['Model']}: {best_base_model['Variance']:.3f})\\n\"\n",
    "        else:\n",
    "            md_content += f\"1. {best_base_model['Model']} has lower variance than ensembles\\n\"\n",
    "    \n",
    "    # Find high variance (overfitting) models - only if gap is positive\n",
    "    high_var_models = [m for m in model_analysis if m['Raw_Gap'] > 0.08]\n",
    "    if high_var_models:\n",
    "        md_content += f\"2. {high_var_models[0]['Model']} shows overfitting (gap = +{high_var_models[0]['Raw_Gap']:.3f})\\n\"\n",
    "    \n",
    "    # Find high bias (underfitting) models  \n",
    "    high_bias_models = [m for m in model_analysis if m['Bias'] > 0.15]\n",
    "    if high_bias_models:\n",
    "        md_content += f\"3. {high_bias_models[0]['Model']} shows higher bias (bias = {high_bias_models[0]['Bias']:.3f})\\n\"\n",
    "    \n",
    "    md_content += f\"4. **{best_model}** shows the best overall bias-variance tradeoff\\n\"\n",
    "    \n",
    "    # Ensemble benefit analysis\n",
    "    if 'Voting_Ensemble' in model_results and 'Stacking_Ensemble' in model_results:\n",
    "        voting_r2 = model_results['Voting_Ensemble'].get('R2_Test', 0)\n",
    "        stacking_r2 = model_results['Stacking_Ensemble'].get('R2_Test', 0)\n",
    "        md_content += f\"\\n**Ensemble Comparison:**\\n\"\n",
    "        md_content += f\"- Voting Ensemble RÂ² = {voting_r2:.4f}\\n\"\n",
    "        md_content += f\"- Stacking Ensemble RÂ² = {stacking_r2:.4f}\\n\"\n",
    "        if stacking_r2 > voting_r2:\n",
    "            md_content += f\"- Stacking outperforms Voting by {(stacking_r2-voting_r2)*100:.2f}%\\n\"\n",
    "        else:\n",
    "            md_content += f\"- Voting outperforms Stacking by {(voting_r2-stacking_r2)*100:.2f}%\\n\"\n",
    "    \n",
    "    return md_content\n",
    "\n",
    "# Generate and display\n",
    "display(Markdown(generate_bias_variance_summary(model_results, best_estimators, X_train, y_train)))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
