{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19261fcd",
   "metadata": {},
   "source": [
    "## 7. Advanced Experimentation: Deep Feature Engineering & Extended Hyperparameter Search\n",
    "\n",
    "Let's push further with:\n",
    "1. **Advanced Feature Transformations**: Log, polynomial, binning, target encoding\n",
    "2. **Extended Hyperparameter Grids**: Broader search space\n",
    "3. **RandomizedSearchCV**: Search more parameters efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d3ad23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7.1 Advanced Feature Engineering V2 ---\n",
    "print(\"=\"*70)\n",
    "print(\"ADVANCED FEATURE ENGINEERING V2\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures, PowerTransformer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "df_v2 = df.copy()\n",
    "\n",
    "# --- Feature 1: Log transformations (handle skewed distributions) ---\n",
    "df_v2['log_size'] = np.log1p(df_v2['size'])\n",
    "df_v2['log_room_count'] = np.log1p(df_v2['room_count'])\n",
    "\n",
    "# --- Feature 2: Ratios and proportions ---\n",
    "df_v2['bathroom_ratio'] = df_v2['bathroom_count'] / (df_v2['room_count'] + 1)  # bathroom per room\n",
    "df_v2['size_per_bathroom'] = df_v2['size'] / (df_v2['bathroom_count'] + 1)\n",
    "df_v2['room_density'] = df_v2['room_count'] / (df_v2['size'] / 100)  # rooms per 100mÂ²\n",
    "\n",
    "# --- Feature 3: Binned features (categorical from continuous) ---\n",
    "df_v2['size_category'] = pd.cut(df_v2['size'], bins=[0, 60, 100, 150, 250, 600], \n",
    "                                 labels=['studio', 'small', 'medium', 'large', 'luxury'])\n",
    "df_v2['size_category'] = df_v2['size_category'].astype(str)\n",
    "\n",
    "# --- Feature 4: Target encoding for city (mean price per city) ---\n",
    "city_price_mean = df_v2.groupby('city')['price'].transform('mean')\n",
    "city_price_std = df_v2.groupby('city')['price'].transform('std')\n",
    "df_v2['city_price_mean'] = city_price_mean\n",
    "df_v2['city_price_std'] = city_price_std\n",
    "\n",
    "# --- Feature 5: Polynomial features (size^2) ---\n",
    "df_v2['size_squared'] = df_v2['size'] ** 2\n",
    "df_v2['size_cubed'] = df_v2['size'] ** 3 / 10000  # scaled\n",
    "\n",
    "print(\"New features created:\")\n",
    "new_features = ['log_size', 'log_room_count', 'bathroom_ratio', 'size_per_bathroom', \n",
    "                'room_density', 'size_category', 'tier_x_size', 'tier_x_rooms',\n",
    "                'city_price_mean', 'city_price_std', 'size_squared', 'size_cubed']\n",
    "print(f\"  {new_features}\")\n",
    "print(f\"\\nTotal new features: {len(new_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a290df50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7.2 Prepare Enhanced Dataset V2 ---\n",
    "\n",
    "# Define features for V2 model\n",
    "features_v2 = ['city', 'size', 'room_count', 'bathroom_count', 'avg_room_size',\n",
    "               'log_size', 'bathroom_ratio', 'size_per_bathroom', 'room_density',\n",
    "               'size_category' , \n",
    "               'city_price_mean', 'size_squared']\n",
    "\n",
    "X_v2 = df_v2[features_v2]\n",
    "y_v2 = df_v2['log_price']\n",
    "\n",
    "# Updated preprocessing for V2\n",
    "categorical_v2 = ['city', 'size_category']\n",
    "numeric_v2 = [f for f in features_v2 if f not in categorical_v2]\n",
    "\n",
    "preprocessor_v2 = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', RobustScaler(), numeric_v2),\n",
    "        ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), categorical_v2)\n",
    "    ])\n",
    "\n",
    "X_train_v2, X_test_v2, y_train_v2, y_test_v2 = train_test_split(\n",
    "    X_v2, y_v2, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Features V2: {len(features_v2)} total\")\n",
    "print(f\"  Numeric: {len(numeric_v2)}\")\n",
    "print(f\"  Categorical: {len(categorical_v2)}\")\n",
    "print(f\"Training samples: {len(X_train_v2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685f0804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7.3 Extended Hyperparameter Search with RandomizedSearchCV ---\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXTENDED HYPERPARAMETER SEARCH (RandomizedSearchCV)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Store V2 results\n",
    "v2_results = {}\n",
    "\n",
    "# --- 7.3.1 GradientBoosting with Extended Grid ---\n",
    "print(\"\\n--- GradientBoosting Extended Search ---\")\n",
    "gb_extended_params = {\n",
    "    'model__n_estimators': randint(100, 500),\n",
    "    'model__learning_rate': uniform(0.01, 0.2),\n",
    "    'model__max_depth': randint(3, 10),\n",
    "    'model__min_samples_split': randint(2, 20),\n",
    "    'model__min_samples_leaf': randint(1, 10),\n",
    "    'model__subsample': uniform(0.6, 0.4),\n",
    "    'model__max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "gb_pipeline_v2 = Pipeline([\n",
    "    ('prep', preprocessor_v2),\n",
    "    ('model', GradientBoostingRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "gb_random = RandomizedSearchCV(\n",
    "    gb_pipeline_v2, gb_extended_params, n_iter=50, cv=5, \n",
    "    scoring='r2', n_jobs=-1, random_state=42, return_train_score=True\n",
    ")\n",
    "print(\"Searching 50 random combinations...\")\n",
    "gb_random.fit(X_train_v2, y_train_v2)\n",
    "\n",
    "y_pred_gb_v2 = gb_random.predict(X_test_v2)\n",
    "r2_gb_v2 = r2_score(y_test_v2, y_pred_gb_v2)\n",
    "mae_gb_v2 = mean_absolute_error(np.expm1(y_test_v2), np.expm1(y_pred_gb_v2))\n",
    "\n",
    "gb_cv_train = gb_random.cv_results_['mean_train_score'][gb_random.best_index_]\n",
    "gb_cv_test = gb_random.cv_results_['mean_test_score'][gb_random.best_index_]\n",
    "\n",
    "print(f\"Best Params: {gb_random.best_params_}\")\n",
    "print(f\"RÂ² = {r2_gb_v2:.4f} | MAE = {mae_gb_v2:,.0f} TND\")\n",
    "print(f\"CV Train: {gb_cv_train:.4f} | CV Test: {gb_cv_test:.4f} | Gap: {gb_cv_train-gb_cv_test:.4f}\")\n",
    "\n",
    "v2_results['GB_Extended_V2'] = {\n",
    "    'R2_Test': r2_gb_v2, 'MAE': mae_gb_v2, \n",
    "    'CV_Train': gb_cv_train, 'CV_Test': gb_cv_test,\n",
    "    'Variance_Gap': gb_cv_train - gb_cv_test\n",
    "}\n",
    "best_estimators['GB_Extended_V2'] = gb_random.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f841c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7.3.2 RandomForest Extended (with regularization to reduce overfitting) ---\n",
    "print(\"\\n--- RandomForest Extended Search (Anti-Overfitting Focus) ---\")\n",
    "\n",
    "rf_extended_params = {\n",
    "    'model__n_estimators': randint(100, 400),\n",
    "    'model__max_depth': randint(5, 20),  # Lower depth to reduce overfitting\n",
    "    'model__min_samples_split': randint(5, 30),  # Higher to reduce overfitting\n",
    "    'model__min_samples_leaf': randint(2, 15),\n",
    "    'model__max_features': ['sqrt', 'log2', 0.5, 0.7],\n",
    "    'model__max_samples': uniform(0.6, 0.3)  # Bootstrap sample ratio\n",
    "}\n",
    "\n",
    "rf_pipeline_v2 = Pipeline([\n",
    "    ('prep', preprocessor_v2),\n",
    "    ('model', RandomForestRegressor(random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "rf_random = RandomizedSearchCV(\n",
    "    rf_pipeline_v2, rf_extended_params, n_iter=50, cv=5,\n",
    "    scoring='r2', n_jobs=-1, random_state=42, return_train_score=True\n",
    ")\n",
    "print(\"Searching 50 random combinations...\")\n",
    "rf_random.fit(X_train_v2, y_train_v2)\n",
    "\n",
    "y_pred_rf_v2 = rf_random.predict(X_test_v2)\n",
    "r2_rf_v2 = r2_score(y_test_v2, y_pred_rf_v2)\n",
    "mae_rf_v2 = mean_absolute_error(np.expm1(y_test_v2), np.expm1(y_pred_rf_v2))\n",
    "\n",
    "rf_cv_train = rf_random.cv_results_['mean_train_score'][rf_random.best_index_]\n",
    "rf_cv_test = rf_random.cv_results_['mean_test_score'][rf_random.best_index_]\n",
    "\n",
    "print(f\"Best Params: {rf_random.best_params_}\")\n",
    "print(f\"RÂ² = {r2_rf_v2:.4f} | MAE = {mae_rf_v2:,.0f} TND\")\n",
    "print(f\"CV Train: {rf_cv_train:.4f} | CV Test: {rf_cv_test:.4f} | Gap: {rf_cv_train-rf_cv_test:.4f}\")\n",
    "\n",
    "v2_results['RF_Extended_V2'] = {\n",
    "    'R2_Test': r2_rf_v2, 'MAE': mae_rf_v2,\n",
    "    'CV_Train': rf_cv_train, 'CV_Test': rf_cv_test,\n",
    "    'Variance_Gap': rf_cv_train - rf_cv_test\n",
    "}\n",
    "best_estimators['RF_Extended_V2'] = rf_random.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925d0228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7.3.3 SVR Extended with RBF kernel optimization ---\n",
    "print(\"\\n--- SVR Extended Search ---\")\n",
    "\n",
    "svr_extended_params = {\n",
    "    'model__C': uniform(0.1, 50),\n",
    "    'model__epsilon': uniform(0.01, 0.3),\n",
    "    'model__gamma': ['scale', 'auto'] + list(uniform(0.001, 0.1).rvs(5)),\n",
    "    'model__kernel': ['rbf', 'poly']\n",
    "}\n",
    "\n",
    "svr_pipeline_v2 = Pipeline([\n",
    "    ('prep', preprocessor_v2),\n",
    "    ('model', SVR())\n",
    "])\n",
    "\n",
    "svr_random = RandomizedSearchCV(\n",
    "    svr_pipeline_v2, svr_extended_params, n_iter=40, cv=5,\n",
    "    scoring='r2', n_jobs=-1, random_state=42, return_train_score=True\n",
    ")\n",
    "print(\"Searching 40 random combinations...\")\n",
    "svr_random.fit(X_train_v2, y_train_v2)\n",
    "\n",
    "y_pred_svr_v2 = svr_random.predict(X_test_v2)\n",
    "r2_svr_v2 = r2_score(y_test_v2, y_pred_svr_v2)\n",
    "mae_svr_v2 = mean_absolute_error(np.expm1(y_test_v2), np.expm1(y_pred_svr_v2))\n",
    "\n",
    "svr_cv_train = svr_random.cv_results_['mean_train_score'][svr_random.best_index_]\n",
    "svr_cv_test = svr_random.cv_results_['mean_test_score'][svr_random.best_index_]\n",
    "\n",
    "print(f\"Best Params: {svr_random.best_params_}\")\n",
    "print(f\"RÂ² = {r2_svr_v2:.4f} | MAE = {mae_svr_v2:,.0f} TND\")\n",
    "print(f\"CV Train: {svr_cv_train:.4f} | CV Test: {svr_cv_test:.4f} | Gap: {svr_cv_train-svr_cv_test:.4f}\")\n",
    "\n",
    "v2_results['SVR_Extended_V2'] = {\n",
    "    'R2_Test': r2_svr_v2, 'MAE': mae_svr_v2,\n",
    "    'CV_Train': svr_cv_train, 'CV_Test': svr_cv_test,\n",
    "    'Variance_Gap': svr_cv_train - svr_cv_test\n",
    "}\n",
    "best_estimators['SVR_Extended_V2'] = svr_random.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3783d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7.5 Final Comparison: V1 (Original) vs V2 (Extended) ---\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL COMPARISON: Original (V1) vs Extended (V2)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# V2 Results DataFrame\n",
    "v2_comparison = pd.DataFrame(v2_results).T\n",
    "for col in ['R2_Test', 'MAE']:\n",
    "    if col in v2_comparison.columns:\n",
    "        v2_comparison[col] = pd.to_numeric(v2_comparison[col], errors='coerce')\n",
    "v2_comparison = v2_comparison.sort_values('R2_Test', ascending=False)\n",
    "\n",
    "print(\"\\n--- V2 (Extended Features + Deep Hyperparameter Search) ---\")\n",
    "print(v2_comparison[['R2_Test', 'MAE', 'Variance_Gap']].round(4).to_string())\n",
    "\n",
    "# Overall comparison\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"HEAD-TO-HEAD: Best V1 vs Best V2\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "best_v1_r2 = r2_voting  # Original voting ensemble\n",
    "best_v2_name = v2_comparison['R2_Test'].idxmax()\n",
    "best_v2_r2 = v2_comparison.loc[best_v2_name, 'R2_Test']\n",
    "best_v2_mae = v2_comparison.loc[best_v2_name, 'MAE']\n",
    "\n",
    "print(f\"\\nOriginal Voting Ensemble (V1):\")\n",
    "print(f\"  RÂ² = {best_v1_r2:.4f} | MAE = {mae_voting:,.0f} TND\")\n",
    "\n",
    "print(f\"\\nBest V2 Model ({best_v2_name}):\")\n",
    "print(f\"  RÂ² = {best_v2_r2:.4f} | MAE = {best_v2_mae:,.0f} TND\")\n",
    "\n",
    "improvement = (best_v2_r2 - best_v1_r2) * 100\n",
    "print(f\"\\n{'ðŸš€' if improvement > 0 else 'âš ï¸'} RÂ² Change: {improvement:+.2f}%\")\n",
    "\n",
    "if improvement > 0:\n",
    "    print(f\"\\nâœ… SUCCESS! V2 improved by {improvement:.2f}%\")\n",
    "    print(f\"   New features and extended hyperparameter search helped!\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸ V2 did not outperform V1. Original model remains champion.\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# RÂ² comparison\n",
    "ax1 = axes[0]\n",
    "all_r2 = {'Original_Voting (V1)': best_v1_r2}\n",
    "for name in v2_comparison.index:\n",
    "    all_r2[f'{name}'] = v2_comparison.loc[name, 'R2_Test']\n",
    "    \n",
    "sorted_models = sorted(all_r2.items(), key=lambda x: x[1])\n",
    "names, values = zip(*sorted_models)\n",
    "colors = ['gold' if v == max(values) else ('green' if 'V1' in n else 'steelblue') for n, v in sorted_models]\n",
    "ax1.barh(names, values, color=colors)\n",
    "ax1.axvline(x=best_v1_r2, color='red', linestyle='--', alpha=0.7, label=f'V1 Baseline: {best_v1_r2:.4f}')\n",
    "ax1.set_xlabel('RÂ² Score')\n",
    "ax1.set_title('V1 vs V2 Model Comparison')\n",
    "ax1.legend()\n",
    "\n",
    "# Variance Gap (bias-variance tradeoff)\n",
    "ax2 = axes[1]\n",
    "variance_data = {k: v.get('Variance_Gap', np.nan) for k, v in v2_results.items() if 'Variance_Gap' in v}\n",
    "if variance_data:\n",
    "    names_var = list(variance_data.keys())\n",
    "    gaps = list(variance_data.values())\n",
    "    colors_var = ['red' if g > 0.05 else 'green' for g in gaps]\n",
    "    ax2.barh(names_var, gaps, color=colors_var)\n",
    "    ax2.axvline(x=0.05, color='orange', linestyle='--', label='Overfitting Threshold')\n",
    "    ax2.set_xlabel('Variance Gap (Train RÂ² - Test RÂ²)')\n",
    "    ax2.set_title('Bias-Variance Analysis (Lower = Better)')\n",
    "    ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
