\documentclass[12pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\usepackage{enumitem}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow}

% Page geometry
\geometry{left=2.5cm, right=2.5cm, top=2.5cm, bottom=2.5cm}

% Colors
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

% Code listing style
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle}

% Hyperref setup
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    citecolor=blue,
}

% Title information
\title{
    \textbf{House Price Prediction in Grand Tunis:} \\
    \large A Comprehensive Machine Learning Approach \\
    \vspace{0.5cm}
    \large Data Mining and Machine Learning Project Report
}
\author{
    Mohamed Aziz Dhouibi\\
    Mohamed Achref Hemissi\\
    Rayen Chemlali\\
    Mohamed Dhia Medini\\
    Khalil Ghimaji\\
    \small Data Mining and Machine Learning Course\\
    \small GL4 - DMML\\
    \small \href{https://github.com/AchrefHemissi/House-Prices-in-Grand-Tunis-Data-Mining}{GitHub Repository}
}

\begin{document}

\maketitle
\newpage
\begin{abstract}
This report presents a comprehensive machine learning solution for predicting apartment prices in the Grand Tunis region. We developed two model versions: v1 built on raw unprocessed data, and v2 built on preprocessed and merged data from multiple sources. Through systematic feature engineering, advanced ensemble methods, and rigorous hyperparameter optimization, we achieved an R² score exceeding 0.80, placing our models in the top tier of real estate valuation systems. Key innovations include KNN-based region imputation, city-specific outlier removal, and a production-ready deployment pipeline. Our results demonstrate that with limited features, machine learning can explain over 80\% of price variance in the real estate market, providing a practical tool for property valuation and market analysis.
\end{abstract}
\newpage
\tableofcontents
\newpage

%==============================================================================
\section{Introduction}
%==============================================================================

\subsection{Problem Statement}

Real estate price prediction is a critical problem in urban planning, investment decision-making, and market analysis. In the Grand Tunis region, comprising Tunis, Ariana, Ben Arous, and La Manouba, apartment prices vary significantly based on location, size, and property characteristics. Accurate price prediction models can:

\begin{itemize}
    \item Help buyers and sellers make informed decisions
    \item Assist real estate agencies in property valuation
    \item Enable market trend analysis and forecasting
    \item Support urban development planning
\end{itemize}

\subsection{Objectives}

The primary objectives of this project are:

\begin{enumerate}
    \item Develop accurate machine learning models for apartment price prediction
    \item Compare model performance on raw vs. preprocessed data
    \item Engineer meaningful features that capture price determinants
    \item Create a production-ready deployment pipeline
    \item Achieve R² score $\geq$ 0.75 (industry standard for real estate)
\end{enumerate}

\subsection{Dataset Overview}

We worked with two data sources:

\begin{itemize}
    \item \textbf{Source 1}: Property-Prices-in-Tunisia.csv (raw real estate listings)
    \item \textbf{Source 2}: data\_prices\_cleaned.csv (preprocessed listings)
    \item \textbf{Merged Dataset}: Combined and cleaned data from both sources
\end{itemize}

\textbf{Key Features}:
\begin{itemize}
    \item Numerical: \texttt{size} (m²), \texttt{room\_count}, \texttt{bathroom\_count}, \texttt{price} (TND)
    \item Categorical: \texttt{city} (4 cities), \texttt{region} (30+ neighborhoods)
\end{itemize}

%==============================================================================
\section{Data Collection and Preprocessing}
%==============================================================================

\subsection{Data Sources}

\subsubsection{Source 1: Property Prices in Tunisia}

\begin{table}[H]
\centering
\caption{Source 1 Dataset Characteristics}
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Property} & \textbf{Value} \\ \midrule
Author & Ghassen Chaabouni \\
Platform & Kaggle \\
File & Property-Prices-in-Tunisia.csv \\
Size & 1.05 MB \\
Total Records & 12,749 \\
Collection Period & January 22 - February 26, 2020 \\
Collection Method & Web Scraping \\
Geographic Coverage & Tunisia (nationwide) \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Original Schema}
The raw dataset contained 9 columns:

\begin{table}[H]
\centering
\caption{Source 1 Original Column Schema}
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Column} & \textbf{Type} & \textbf{Description} \\ \midrule
\texttt{room\_count} & Numeric & Number of rooms \\
\texttt{bathroom\_count} & Numeric & Number of bathrooms \\
\texttt{size} & Numeric & Property size (m²) \\
\texttt{price} & Numeric & Price in TND \\
\texttt{city} & Categorical & City location \\
\texttt{region} & Categorical & Neighborhood/area \\
\texttt{category} & Categorical & Property type \\
\texttt{type} & Categorical & À Vendre/À Louer \\
\texttt{log\_price} & Numeric & Log-transformed price \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Source 2: Tunisian Real Estate Dataset}

\begin{table}[H]
\centering
\caption{Source 2 Dataset Characteristics}
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Property} & \textbf{Value} \\ \midrule
Author & Debbichi Raaki \\
Platform & Kaggle (Tayara scraping) \\
File & data\_prices\_cleaned.csv \\
Size & 5.48 MB \\
Total Records & 8,336 \\
Collection Period & February 14 - April 12, 2024 \\
Collection Method & Web scraping from Tayara \\
Geographic Coverage & Tunisia (all regions) \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Original Schema}
The raw dataset contained 17 columns including detailed listing information:

\begin{table}[H]
\centering
\caption{Source 2 Original Column Schema (Selected)}
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Column} & \textbf{Type} & \textbf{Description} \\ \midrule
\texttt{chambres} & Numeric & Number of bedrooms \\
\texttt{salles\_de\_bains} & Numeric & Number of bathrooms \\
\texttt{superficie} & Numeric & Property size (m²) \\
\texttt{price} & Numeric & Price in TND \\
\texttt{state} & Categorical & Governorate \\
\texttt{city} & Categorical & City/District \\
\texttt{category} & Categorical & Property type \\
\texttt{transaction} & Categorical & Sale/Rent \\
\texttt{date} & DateTime & Listing date \\
\texttt{titles} & Text & Listing title \\
\texttt{descriptions} & Text & Property description \\
\texttt{contact} & Text & Contact information \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Source 1 Preprocessing Pipeline}

The Source 1 preprocessing was executed through 5 sequential Jupyter notebooks:

\subsubsection{01: Data Loading and Setup}

\begin{itemize}
    \item Imported essential libraries: \texttt{pandas}, \texttt{matplotlib}, \texttt{seaborn}, \texttt{numpy}
    \item Loaded \texttt{Property-Prices-in-Tunisia.csv}
    \item Performed initial data inspection and statistical overview
\end{itemize}

\subsubsection{02: Data Cleaning and Preprocessing}

\paragraph{Geographic Filtering}
Limited data to Grand Tunis region exclusively:
\begin{itemize}
    \item Tunis (capital)
    \item Ariana
    \item Ben Arous
    \item La Manouba
\end{itemize}

\paragraph{Property Type Filtering}
\begin{itemize}
    \item Category: \textbf{Appartements} only
    \item Listing Type: \textbf{À Vendre} (For Sale) only
\end{itemize}

\paragraph{Price Transformation}
Converted prices from TND to kTND (thousands):
\begin{equation}
price_{kTND} = \frac{price_{TND}}{1000}
\end{equation}

\paragraph{Outlier Removal Strategy}
Applied multiple statistical filters:

\begin{table}[H]
\centering
\caption{Source 1 Outlier Removal Criteria}
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Filter} & \textbf{Condition} & \textbf{Rationale} \\ \midrule
High price cap & price $>$ 3,000 kTND & Remove luxury outliers \\
Underpriced large & price $\leq$ 70 kTND AND size $\geq$ 70m² & Unrealistic cheap apartments \\
Overpriced small & price $\geq$ 1,000 kTND AND size $\leq$ 90m² & Unrealistic expensive small units \\
Impossible config & rooms $\geq$ 2 AND size $\leq$ 25m² & Physically impossible \\
Price/m² filter & price/m² $>$ 6,000 TND & Extreme value outliers \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Duplicate Removal}
\begin{itemize}
    \item \textbf{Exact Duplicates}: Removed completely identical rows
    \item \textbf{Partial Duplicates}: Identified based on key features (\texttt{room\_count}, \texttt{bathroom\_count}, \texttt{size}, \texttt{city}, \texttt{region}), excluding price variations
\end{itemize}

\paragraph{Column Cleanup}
Removed unnecessary columns:
\begin{itemize}
    \item \texttt{category} (already filtered to Appartements)
    \item \texttt{type} (already filtered to À Vendre)
    \item \texttt{log\_price} (temporary analytical column)
\end{itemize}

\subsubsection{03: Feature Distribution Analysis}

Performed comprehensive exploratory data analysis:

\begin{itemize}
    \item \textbf{Size Distribution}: Histogram with KDE, outlier identification
    \item \textbf{Room Count}: Count plot and price statistics by room count
    \item \textbf{Bathroom Count}: Distribution analysis and price relationships
    \item \textbf{Geographic Distribution}: Property counts per city and region
\end{itemize}

\subsubsection{04: Price Analysis}

Conducted detailed price relationship studies:

\begin{itemize}
    \item Overall price distribution (log-transformed box plots)
    \item Price vs Size regression analysis
    \item Price distribution by region and city (log-transformed)
    \item Price variation by bathroom and room count
    \item Price per m² distribution with median analysis
    \item Budget apartments analysis (below 80 kTND)
    \item Luxury apartments analysis (above 1,000 kTND)
\end{itemize}

\subsubsection{05: Correlation Analysis and Export}

\begin{itemize}
    \item Calculated correlation matrix for numeric features
    \item Generated correlation heatmap visualization
    \item Exported cleaned data to \texttt{apartments\_cleaned.csv}
\end{itemize}

\paragraph{Final Output Statistics}

\begin{table}[H]
\centering
\caption{Source 1 Processing Results}
\begin{tabular}{@{}lrr@{}}
\toprule
\textbf{Metric} & \textbf{Before} & \textbf{After} \\ \midrule
Total Records & 12,749 & $\sim$947 \\
Data Retention & 100\% & $\sim$7.4\% \\
Geographic Scope & All Tunisia & Grand Tunis only \\
Property Types & All categories & Apartments (sale) only \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Source 2 Preprocessing Pipeline}

The Source 2 preprocessing was executed through 6 sequential Jupyter notebooks:

\subsubsection{01: Data Loading and Setup}

\begin{itemize}
    \item Imported essential libraries
    \item Loaded \texttt{data\_prices\_cleaned.csv}
    \item Initial data inspection
\end{itemize}

\subsubsection{02: Data Cleaning and Preprocessing}

\paragraph{Numerical Column Cleaning}
\begin{itemize}
    \item Cleaned \texttt{superficie}, \texttt{chambres}, \texttt{salles\_de\_bains}, \texttt{price}
    \item Removed spaces and converted comma decimals to dots
    \item Handled non-numeric values (converted to NaN)
\end{itemize}

\paragraph{Column Renaming for Consistency}
\begin{align}
superficie &\rightarrow size \\
chambres &\rightarrow room\_count \\
salles\_de\_bains &\rightarrow bathroom\_count
\end{align}

\paragraph{Geographic Filtering}
Limited to Grand Tunis governorates:
\begin{itemize}
    \item Tunis
    \item Ariana
    \item Ben Arous
    \item La Manouba
\end{itemize}

\paragraph{Property Type Filtering}
\begin{itemize}
    \item Category: \textbf{Appartements} exclusively
    \item Transaction: \textbf{sale} status only
\end{itemize}

\paragraph{Price Transformation}
\begin{equation}
price_{kTND} = \frac{price_{TND}}{1000}
\end{equation}

\paragraph{Outlier Removal Criteria}

\begin{table}[H]
\centering
\caption{Source 2 Outlier Removal Filters}
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Feature} & \textbf{Filter Condition} \\ \midrule
Size & 24 m² $\leq$ size $<$ 500 m² \\
Price & price $>$ 20 kTND \\
Price/Size Ratio & price/size $\leq$ 6 \\
Room Count & 0 $<$ rooms $<$ 10 \\
Bathroom Count & bathrooms $\geq$ 0 \\
\bottomrule
\end{tabular}
\end{table}

Additional anomaly removal:
\begin{itemize}
    \item Large properties with suspiciously low prices
    \item Small properties with extremely high prices
\end{itemize}

\paragraph{Column Removal}
Dropped irrelevant columns:
\begin{itemize}
    \item \texttt{contact}, \texttt{category}, \texttt{location}, \texttt{descriptions}
    \item \texttt{currency}, \texttt{date}, \texttt{transaction}, \texttt{titles}
    \item \texttt{shops}, \texttt{profiles}
\end{itemize}

\paragraph{Data Validation}
Removed rows with missing values in key columns:
\begin{itemize}
    \item \texttt{price}
    \item \texttt{size}
    \item \texttt{room\_count}
    \item \texttt{bathroom\_count}
\end{itemize}

\subsubsection{03: Feature Distribution Analysis}

\begin{itemize}
    \item Size distribution with histograms and KDE
    \item Identification of properties $>$ 250 m² and $>$ 400 m²
    \item Room count distribution with price statistics
    \item Bathroom count analysis
    \item Geographic distribution across cities and regions
\end{itemize}

\subsubsection{04: Price Analysis}

\begin{itemize}
    \item Basic price statistics and outlier identification
    \item Price vs Size regression plot (log-transformed)
    \item Overall price distribution (box plot, log scale)
    \item Geographic price analysis per region and city
    \item Price distribution by bathroom and room count
    \item Price per square meter analysis
\end{itemize}

\subsubsection{05: Model Training and Evaluation}

\paragraph{Data Preparation}
\begin{itemize}
    \item Features: \texttt{room\_count}, \texttt{bathroom\_count}, \texttt{size}
    \item Target: \texttt{price} (kTND)
    \item Train-test split: 80\%/20\%
    \item Feature scaling with StandardScaler
\end{itemize}

\paragraph{Model Configuration}
\begin{itemize}
    \item Algorithm: XGBoost Regressor
    \item Hyperparameters:
    \begin{itemize}
        \item n\_estimators: 100
        \item learning\_rate: 0.1
        \item max\_depth: 5
        \item subsample: 0.8
        \item colsample\_bytree: 0.8
    \end{itemize}
\end{itemize}

\paragraph{Evaluation Metrics}
\begin{itemize}
    \item MAE, RMSE, R² for training and test sets
    \item 5-fold cross-validation
    \item Feature importance ranking
    \item Residual analysis
\end{itemize}

\subsubsection{06: Data Export}

\paragraph{Final Column Selection}
\begin{itemize}
    \item \texttt{room\_count}
    \item \texttt{bathroom\_count}
    \item \texttt{size}
    \item \texttt{price} (kTND)
    \item \texttt{state} $\rightarrow$ \texttt{city}
    \item \texttt{city} $\rightarrow$ \texttt{region}
\end{itemize}

\paragraph{Export}
\begin{itemize}
    \item File: \texttt{processed\_apartment\_data.csv}
    \item Location: \texttt{data/processed/source\_2/}
    \item Format: CSV (no index)
\end{itemize}

\paragraph{Processing Statistics}

\begin{table}[H]
\centering
\caption{Source 2 Processing Results}
\begin{tabular}{@{}lrr@{}}
\toprule
\textbf{Metric} & \textbf{Before} & \textbf{After} \\ \midrule
Total Records & 8,336 & 566 \\
Data Retention & 100\% & 6.8\% \\
File Size & 5.48 MB & 21.07 KB \\
Geographic Scope & All Tunisia & Grand Tunis only \\
Property Types & All categories & Apartments (sale) only \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Data Aggregation and Merging}

\subsubsection{Aggregation Strategy}

The two cleaned datasets were merged to create a unified dataset, accounting for temporal differences between 2020 (Source 1) and 2024 (Source 2) data.

\paragraph{Inflation Adjustment}

Source 1 prices were adjusted for inflation:

\begin{equation}
price_{2024} = price_{2020} \times 1.25
\end{equation}

\textbf{Rationale}: Based on Institut National de la Statistique (INS) data, apartment prices in Tunisia increased by 25\% from 2020 to 2024.

\paragraph{Schema Alignment}

Both datasets had compatible schemas after preprocessing:

\begin{table}[H]
\centering
\caption{Unified Schema for Merging}
\begin{tabular}{@{}llcc@{}}
\toprule
\textbf{Column} & \textbf{Type} & \textbf{Source 1} & \textbf{Source 2} \\ \midrule
\texttt{room\_count} & Numeric & \checkmark & \checkmark \\
\texttt{bathroom\_count} & Numeric & \checkmark & \checkmark \\
\texttt{size} & Numeric & \checkmark & \checkmark \\
\texttt{price} & Numeric & \checkmark (adjusted) & \checkmark \\
\texttt{city} & Categorical & \checkmark & \checkmark \\
\texttt{region} & Categorical & \checkmark & \checkmark \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Duplicate Removal Process}

\begin{algorithm}[H]
\caption{Cross-Source Duplicate Detection}
\begin{algorithmic}[1]
\STATE Define common columns: \texttt{[room\_count, bathroom\_count, size, price, city]}
\STATE (Exclude \texttt{region} to preserve regional variations)
\STATE Perform inner merge on common columns
\STATE Identify duplicate entries between sources
\STATE Filter Source 1 to remove duplicates
\STATE Concatenate filtered Source 1 with Source 2
\end{algorithmic}
\end{algorithm}

\subsubsection{Merged Dataset Statistics}

\begin{table}[H]
\centering
\caption{Merged Dataset Characteristics}
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Property} & \textbf{Value} \\ \midrule
Output File & merged.csv \\
Location & data/processed/ \\
Total Records & 1,513 \\
Unique Cities & 4 (Tunis, Ariana, Ben Arous, La Manouba) \\
Unique Regions & 95 \\
Price Range & 47 kTND - 1,875 kTND \\
Size Range & 30 m² - 500 m² \\
Room Range & 1 - 8 rooms \\
Missing Values & 0 (complete dataset) \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Quality Assurance}

\begin{itemize}
    \item \textbf{Data Completeness}: No missing values in any column
    \item \textbf{Geographic Coverage}: Comprehensive coverage across Grand Tunis
    \item \textbf{Temporal Consistency}: Inflation adjustment ensures price comparability
    \item \textbf{Duplicate-Free}: Cross-source duplicates removed
    \item \textbf{Outlier-Controlled}: Both sources had outliers removed pre-merge
\end{itemize}

\subsection{Final Preprocessed Dataset Summary}

\begin{table}[H]
\centering
\caption{Complete Data Processing Pipeline Summary}
\begin{tabular}{@{}lrrrr@{}}
\toprule
\textbf{Stage} & \textbf{Source 1} & \textbf{Source 2} & \textbf{Merged} \\ \midrule
Raw Records & 12,749 & 8,336 & - \\
After Filtering & $\sim$947 & 566 & - \\
After Merging & - & - & 1,513 \\
\midrule
Retention Rate & 7.4\% & 6.8\% & - \\
Combined Contribution & 62.6\% & 37.4\% & 100\% \\
\bottomrule
\end{tabular}
\end{table}

The preprocessing pipeline successfully:
\begin{itemize}
    \item Standardized heterogeneous data sources
    \item Removed statistical outliers while preserving market diversity
    \item Adjusted for temporal price inflation
    \item Created a clean, unified dataset ready for modeling
    \item Maintained data quality with zero missing values
\end{itemize}

%==============================================================================
\section{Methodology}
%==============================================================================

\subsection{Project Architecture}

We developed two parallel pipelines to evaluate the impact of data preprocessing:

\begin{figure}[H]
\centering
\begin{verbatim}
Version 1 (v1)                    Version 2 (v2)
──────────────────               ──────────────────
Raw Data                         Preprocessed Data
    ↓                                   ↓
IQR Outlier Removal              Advanced Cleaning
    ↓                                   ↓
KNN Region Imputation            Region Standardization
    ↓                                   ↓
Feature Engineering              Enhanced Feature Engineering
    ↓                                   ↓
Model Training                   Extended Model Training
    ↓                                   ↓
Ensemble Methods                 Advanced Ensembles
\end{verbatim}
\caption{Dual Pipeline Architecture}
\end{figure}

\subsection{Data Preprocessing}

\subsubsection{Statistical Outlier Removal}

We implemented an IQR-based outlier detection method applied \textit{per city} to respect local market variations:

\begin{algorithm}[H]
\caption{City-Specific IQR Outlier Removal}
\begin{algorithmic}[1]
\STATE Compute $price\_per\_m^2 = \frac{price}{size}$ for each property
\FOR{each city in [Tunis, Ariana, Ben Arous, La Manouba]}
    \STATE $Q_1 \gets 25^{th}$ percentile of $price\_per\_m^2$
    \STATE $Q_3 \gets 75^{th}$ percentile of $price\_per\_m^2$
    \STATE $IQR \gets Q_3 - Q_1$
    \STATE $lower\_bound \gets Q_1 - 1.5 \times IQR$
    \STATE $upper\_bound \gets Q_3 + 1.5 \times IQR$
    \STATE Remove properties where $price\_per\_m^2 < lower\_bound$ OR $price\_per\_m^2 > upper\_bound$
\ENDFOR
\end{algorithmic}
\end{algorithm}

\textbf{Impact}: Removed approximately 15-20\% of data points while preserving legitimate price variations across different neighborhoods and property types.

\subsubsection{KNN-Based Region Imputation}

Many properties were labeled with generic regions like "Autres villes" (other cities). We developed an innovative KNN-based imputation strategy:

\begin{algorithm}[H]
\caption{Intelligent Region Imputation}
\begin{algorithmic}[1]
\STATE \textbf{Training Phase (per city):}
\STATE $X_{train} \gets$ [size, room\_count, bathroom\_count, price\_per\_m²] for known regions
\STATE $y_{train} \gets$ actual region labels
\STATE Determine optimal $k$ via 5-fold cross-validation
\STATE Train KNN classifier with optimal $k$
\STATE
\STATE \textbf{Imputation Phase:}
\FOR{each property with region = "Autres villes"}
    \STATE $X_{new} \gets$ [size, room\_count, bathroom\_count, price\_per\_m²]
    \STATE $\hat{y} \gets$ KNN.predict($X_{new}$)
    \STATE Assign imputed region $\hat{y}$ to property
\ENDFOR
\end{algorithmic}
\end{algorithm}

This approach leveraged property characteristics to predict the most likely neighborhood, significantly improving model accuracy.

\subsection{Feature Engineering}

\subsubsection{Base Features}

\begin{table}[H]
\centering
\caption{Feature Set Description}
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Feature} & \textbf{Type} & \textbf{Description} \\ \midrule
\texttt{size} & Numeric & Property size in m² \\
\texttt{room\_count} & Numeric & Number of rooms \\
\texttt{bathroom\_count} & Numeric & Number of bathrooms \\
\texttt{city} & Categorical & City location (4 categories) \\
\texttt{region} & Categorical & Neighborhood (30+ categories) \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Derived Features}

\begin{equation}
price\_per\_m^2 = \frac{price}{size}
\end{equation}

\begin{equation}
avg\_room\_size = \frac{size}{room\_count}
\end{equation}

These derived features capture:
\begin{itemize}
    \item \textbf{Price per m²}: Market value density indicator
    \item \textbf{Average room size}: Property spaciousness metric
\end{itemize}

\subsection{Model Selection}

\subsubsection{Base Models Evaluated}

We systematically evaluated six regression algorithms:

\begin{table}[H]
\centering
\caption{Machine Learning Models Evaluated}
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Model} & \textbf{Type} & \textbf{Key Parameters} \\ \midrule
Ridge Regression & Linear & $\alpha$ (regularization) \\
Random Forest & Ensemble & $n_{estimators}$, $max\_depth$ \\
Gradient Boosting & Ensemble & $learning\_rate$, $n_{estimators}$ \\
SVR (RBF kernel) & Kernel-based & $C$, $\gamma$, $epsilon$ \\
AdaBoost & Ensemble & $n_{estimators}$, $learning\_rate$ \\
XGBoost & Gradient Boosting & $max\_depth$, $\eta$, $\lambda$ \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Hyperparameter Optimization}

We employed \texttt{RandomizedSearchCV} with 5-fold cross-validation to optimize hyperparameters:

\textbf{Random Forest}:
\begin{itemize}
    \item \texttt{n\_estimators}: $\{100, 200, 300, 500\}$
    \item \texttt{max\_depth}: $\{10, 20, 30, None\}$
    \item \texttt{min\_samples\_split}: $\{2, 5, 10\}$
    \item \texttt{min\_samples\_leaf}: $\{1, 2, 4\}$
\end{itemize}

\textbf{Gradient Boosting}:
\begin{itemize}
    \item \texttt{n\_estimators}: $\{100, 200, 300, 500\}$
    \item \texttt{learning\_rate}: $\{0.01, 0.05, 0.1, 0.2\}$
    \item \texttt{max\_depth}: $\{3, 5, 7, 10\}$
    \item \texttt{subsample}: $\{0.8, 0.9, 1.0\}$
\end{itemize}

\textbf{SVR}:
\begin{itemize}
    \item \texttt{C}: $\{0.1, 1, 10, 100, 1000\}$
    \item \texttt{gamma}: $\{scale, auto, 0.001, 0.01, 0.1\}$
    \item \texttt{kernel}: RBF
\end{itemize}

\subsubsection{Ensemble Strategies}

\paragraph{Voting Ensemble}
Combines predictions from multiple models via averaging:

\begin{equation}
\hat{y}_{voting} = \frac{1}{N} \sum_{i=1}^{N} \hat{y}_i
\end{equation}

where $N$ is the number of models and $\hat{y}_i$ is the prediction from model $i$.

\textbf{Our Configuration}: Gradient Boosting + SVR + Random Forest

\paragraph{Weighted Voting Ensemble}
Assigns weights based on cross-validation performance:

\begin{equation}
\hat{y}_{weighted} = \frac{\sum_{i=1}^{N} w_i \cdot \hat{y}_i}{\sum_{i=1}^{N} w_i}
\end{equation}

where $w_i$ is the weight for model $i$ (typically proportional to its R² score).

\paragraph{Elite Ensemble (v2 Innovation)}
Uses only the two most stable models with lowest variance gap:

\begin{itemize}
    \item SVR (variance gap: 0.044)
    \item Gradient Boosting (variance gap: 0.050)
\end{itemize}

By excluding high-variance models (e.g., Random Forest with gap 0.115), we achieved better generalization.

\subsection{Evaluation Metrics}

\subsubsection{Primary Metrics}

\paragraph{R² Score (Coefficient of Determination)}
\begin{equation}
R^2 = 1 - \frac{\sum_{i=1}^{n}(y_i - \hat{y}_i)^2}{\sum_{i=1}^{n}(y_i - \bar{y})^2}
\end{equation}

Measures proportion of variance explained by the model. Target: $R^2 \geq 0.75$

\paragraph{Mean Absolute Error (MAE)}
\begin{equation}
MAE = \frac{1}{n}\sum_{i=1}^{n}|y_i - \hat{y}_i|
\end{equation}

Average absolute prediction error in TND. Lower is better.

\paragraph{Root Mean Squared Error (RMSE)}
\begin{equation}
RMSE = \sqrt{\frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2}
\end{equation}

Penalizes large errors more heavily than MAE.

\subsubsection{Bias-Variance Analysis}

\paragraph{Variance Gap}
\begin{equation}
Variance\_Gap = R^2_{train} - R^2_{test}
\end{equation}

Target: $Variance\_Gap < 0.05$ (indicates good generalization)

A large gap suggests overfitting, while a negative gap may indicate underfitting or data issues.

%==============================================================================
\section{Results}
%==============================================================================

\subsection{Version 1 (v1) - Raw Data Pipeline}

\subsubsection{Data Statistics}

\begin{table}[H]
\centering
\caption{Version 1 Dataset Statistics}
\begin{tabular}{@{}lrrr@{}}
\toprule
\textbf{Metric} & \textbf{Before Cleaning} & \textbf{After Cleaning} & \textbf{Change} \\ \midrule
Total Properties & $\sim$5,000 & $\sim$4,000 & -20\% \\
Average Price (TND) & 520,000 & 485,000 & -6.7\% \\
Average Size (m²) & 142 & 135 & -4.9\% \\
Price/m² (TND) & 3,662 & 3,593 & -1.9\% \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Model Performance}

\begin{table}[H]
\centering
\caption{Version 1 Model Performance Comparison}
\begin{tabular}{@{}lrrrr@{}}
\toprule
\textbf{Model} & \textbf{R²} & \textbf{MAE (TND)} & \textbf{RMSE (TND)} & \textbf{Var. Gap} \\ \midrule
Ridge & 0.7245 & 48,320 & 63,450 & 0.032 \\
Random Forest & 0.7712 & 44,180 & 57,920 & 0.115 \\
Gradient Boosting & 0.7924 & 41,250 & 55,100 & 0.050 \\
SVR (RBF) & 0.7843 & 42,890 & 56,340 & 0.044 \\
AdaBoost & 0.7456 & 46,540 & 60,230 & 0.058 \\
XGBoost & 0.7895 & 41,680 & 55,560 & 0.063 \\
\midrule
\textbf{Voting Ensemble} & \textbf{0.8023} & \textbf{39,850} & \textbf{53,890} & \textbf{0.042} \\
Weighted Voting & 0.8018 & 39,920 & 53,950 & 0.043 \\
Elite Ensemble & 0.7988 & 40,120 & 54,230 & 0.039 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Findings}:
\begin{itemize}
    \item Voting Ensemble achieved best R² = 0.8023 (80.23\% variance explained)
    \item MAE of 39,850 TND represents $\sim$7.8\% of average property price
    \item Ensemble methods consistently outperformed individual models
    \item SVR and Gradient Boosting showed excellent bias-variance balance
\end{itemize}

\subsection{Version 2 (v2) - Enhanced Data Pipeline}

\subsubsection{Data Improvements}

\begin{itemize}
    \item Merged data from two sources
    \item Advanced region name cleaning and standardization
    \item Enhanced geographic coverage across Grand Tunis
\end{itemize}

\subsubsection{Model Performance}

\begin{table}[H]
\centering
\caption{Version 2 Model Performance Comparison}
\begin{tabular}{@{}lrrrr@{}}
\toprule
\textbf{Model} & \textbf{R²} & \textbf{MAE (TND)} & \textbf{RMSE (TND)} & \textbf{Var. Gap} \\ \midrule
Ridge & 0.7312 & 47,220 & 62,180 & 0.028 \\
Random Forest & 0.7789 & 42,950 & 56,340 & 0.108 \\
Gradient Boosting & 0.7998 & 39,780 & 53,650 & 0.046 \\
SVR (RBF) & 0.7921 & 40,890 & 54,720 & 0.040 \\
AdaBoost & 0.7534 & 45,340 & 59,450 & 0.055 \\
XGBoost & 0.7965 & 40,120 & 54,120 & 0.058 \\
\midrule
\textbf{Elite Ensemble} & \textbf{0.8089} & \textbf{38,450} & \textbf{52,380} & \textbf{0.038} \\
Voting Ensemble & 0.8074 & 38,760 & 52,620 & 0.041 \\
Weighted Voting & 0.8071 & 38,810 & 52,680 & 0.040 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Improvements over v1}:
\begin{itemize}
    \item Elite Ensemble R² = 0.8089 (+0.66\% improvement)
    \item MAE reduced to 38,450 TND (-1,400 TND improvement)
    \item Variance gap improved to 0.038 (better generalization)
    \item More stable predictions across test folds
\end{itemize}

\subsection{Version Comparison}

\begin{table}[H]
\centering
\caption{Version 1 vs Version 2 - Champion Model Comparison}
\begin{tabular}{@{}lrr@{}}
\toprule
\textbf{Metric} & \textbf{v1 (Voting)} & \textbf{v2 (Elite)} \\ \midrule
R² Score & 0.8023 & 0.8089 \\
MAE (TND) & 39,850 & 38,450 \\
RMSE (TND) & 53,890 & 52,380 \\
Variance Gap & 0.042 & 0.038 \\
\midrule
\textbf{Improvement} & \textbf{Baseline} & \textbf{+0.66\%} \\
\bottomrule
\end{tabular}
\end{table}
%==============================================================================
\section{Analysis \& Discussion}
%==============================================================================

\subsection{Model Performance Assessment}

\subsubsection{Achievement vs Industry Standards}

Our models achieved R² $> 0.80$, which is \textbf{excellent} for real estate prediction:

\begin{table}[H]
\centering
\caption{Industry Benchmark Comparison}
\begin{tabular}{@{}lrl@{}}
\toprule
\textbf{Model Type} & \textbf{Typical R²} & \textbf{Our Performance} \\ \midrule
Basic Linear Models & 0.50 - 0.65 & - \\
Standard ML Models & 0.65 - 0.75 & - \\
Advanced ML Systems & 0.75 - 0.85 & \textbf{0.80 - 0.81} \\
Commercial Platforms & 0.80 - 0.90 & \textcolor{green}{\checkmark} Comparable \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Why 80\% is Excellent}

Real estate prices have inherent unpredictability due to:

\begin{itemize}
    \item \textbf{Negotiation factors}: Buyer urgency, seller motivation, negotiation skills
    \item \textbf{Unmeasured features}: Building age, condition, view quality, floor level, exact GPS coordinates
    \item \textbf{Market psychology}: Emotional factors, perceived value, marketing quality
    \item \textbf{Temporal effects}: Market cycles, seasonal patterns, economic conditions
    \item \textbf{Micro-location}: Exact street, proximity to amenities, noise levels
\end{itemize}

With our limited feature set, explaining 80\% of variance is near-optimal.

\subsection{Innovation Highlights}

\subsubsection{KNN Region Imputation}

Traditional approaches use mode imputation or clustering. Our KNN approach:

\begin{itemize}
    \item Uses property characteristics to predict most likely region
    \item City-specific models respect local market structure
    \item Cross-validation ensures optimal K selection
    \item Improved prediction accuracy by $\sim$3-5\% over mode imputation
\end{itemize}

\subsubsection{City-Specific Outlier Removal}

Instead of global thresholds, we compute IQR per city because:

\begin{itemize}
    \item Price distributions vary significantly across cities
    \item Luxury properties in Tunis are normal; in Ben Arous they're outliers
    \item Preserves legitimate high-value properties
    \item Removes only true anomalies (data errors, extreme outliers)
\end{itemize}

\subsubsection{Elite Ensemble Strategy}

Rather than including all models, we selected only the two most stable:

\begin{itemize}
    \item Lower variance gap (0.038 vs 0.042)
    \item More consistent predictions across folds
    \item Simpler deployment (fewer models to maintain)
    \item Proof that \textit{quality over quantity} in ensemble methods
\end{itemize}

\subsection{Limitations \& Constraints}

\subsubsection{Data Limitations}

\begin{enumerate}
    \item \textbf{Missing Features}: No data on:
    \begin{itemize}
        \item Building age and construction quality
        \item Floor level and elevator availability
        \item Parking spaces and garage
        \item Balcony, terrace, or view quality
        \item Recent renovations
    \end{itemize}
    
    \item \textbf{Geographic Precision}: Region is categorical, not GPS coordinates
    
    \item \textbf{Temporal Information}: No timestamps to model market trends
    
    \item \textbf{External Factors}: Missing:
    \begin{itemize}
        \item School quality ratings
        \item Crime statistics
        \item Public transportation access
        \item Neighborhood amenities
    \end{itemize}
\end{enumerate}
\subsection{Lessons Learned}

\subsubsection{Successful Strategies}

\begin{enumerate}
    \item \textbf{Ensemble Methods}: Consistently outperformed individual models by 2-3\%
    
    \item \textbf{Cross-Validation}: Prevented overfitting; validated generalization
    
    \item \textbf{Feature Engineering}: Simple derived features (price/m², avg\_room\_size) added value
    
    \item \textbf{Data Quality}: Version 2's cleaner data yielded measurable improvements
    
    \item \textbf{Hyperparameter Tuning}: RandomizedSearchCV improved models by 10-15\%
\end{enumerate}

\subsubsection{Failed Experiments}

\begin{enumerate}
    \item \textbf{Over-Complex Features}: Random polynomial features added noise, not signal
    
    \item \textbf{Deep Stacking}: Meta-learners increased complexity without performance gain
    
    \item \textbf{Too Many Ensemble Members}: Including weak models (AdaBoost, Ridge) degraded performance
    
    \item \textbf{Aggressive Regularization}: Over-regularization led to underfitting
    
    \item \textbf{Neural Networks}: Insufficient data for deep learning; overfitting issues
\end{enumerate}

\subsubsection{Key Insights}

\begin{quote}
\textit{"Simplicity and stability beat complexity and over-optimization."}
\end{quote}

\begin{itemize}
    \item 2-3 well-tuned models $>$ 5+ mediocre models
    \item Data quality improvements often beat algorithmic complexity
    \item Domain knowledge (real estate expertise) crucial for feature engineering
    \item Monitor bias-variance tradeoff continuously
    \item Industry context matters: 80\% R² is excellent, don't over-optimize to 82\%
\end{itemize}

%==============================================================================
\section{Production Deployment}
%==============================================================================

\subsection{Deployment Architecture}

We created a complete production-ready package:

\begin{verbatim}
model_export/
├── house_pricing_pipeline.joblib      # Complete trained pipeline
├── pipeline_metadata.json             # Model metadata & performance
└── inference.py                       # Standalone prediction script
\end{verbatim}

\subsection{Inference Pipeline}

\subsubsection{Usage Example}

\begin{lstlisting}[language=Python, caption=Prediction Script Usage]
from model_export.inference import predict_price

# Predict price for a property
result = predict_price(
    city='tunis',
    size=120,
    room_count=3,
    bathroom_count=2,
    region='autres villes'  # Auto-imputed if unknown
)

print(f"Estimated Price: {result['estimated_price_tnd']:,.2f} TND")
print(f"Imputed Region: {result['imputed_region']}")
\end{lstlisting}

\textbf{Output}:
\begin{verbatim}
Estimated Price: 485,320.50 TND
Imputed Region: lac 2
\end{verbatim}

\subsubsection{Pipeline Components}

The exported pipeline includes:

\begin{enumerate}
    \item \textbf{Champion Model}: Best performing ensemble (Elite or Voting)
    \item \textbf{All Trained Models}: 10+ models for comparison/fallback
    \item \textbf{KNN Region Models}: 4 city-specific imputation models
    \item \textbf{Preprocessing Pipelines}: Scalars, encoders, transformers
    \item \textbf{Feature Lists}: Required feature names for validation
    \item \textbf{City Statistics}: Price/m² statistics for inference
\end{enumerate}

\subsection{API Integration (Future Work)}

Proposed REST API design:

\begin{lstlisting}[language=Python, caption=FastAPI Endpoint Proposal]
from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI()

class PropertyInput(BaseModel):
    city: str
    size: float
    room_count: int
    bathroom_count: int
    region: str = "autres villes"

@app.post("/predict")
async def predict_price_api(property: PropertyInput):
    result = predict_price(
        property.city,
        property.size,
        property.room_count,
        property.bathroom_count,
        property.region
    )
    return result
\end{lstlisting}

%==============================================================================
\section{Future Work}
%==============================================================================

\subsection{Data Enhancements}

\begin{enumerate}
    \item \textbf{Geolocation Data}: Integrate GPS coordinates for distance-based features
    \begin{itemize}
        \item Distance to city center
        \item Proximity to metro stations
        \item Nearby schools, hospitals, shopping
    \end{itemize}
    
    \item \textbf{Building Characteristics}:
    \begin{itemize}
        \item Building age and renovation date
        \item Floor level and total floors
        \item Elevator availability
        \item Parking and garage status
    \end{itemize}
    
    \item \textbf{External Data Sources}:
    \begin{itemize}
        \item School quality ratings
        \item Crime statistics by neighborhood
        \item Public transport accessibility indices
        \item Walkability scores
    \end{itemize}
    
    \item \textbf{Temporal Data}:
    \begin{itemize}
        \item Listing timestamps for trend analysis
        \item Seasonal price patterns
        \item Market cycle indicators
    \end{itemize}
\end{enumerate}

\subsection{Model Improvements}

\begin{enumerate}
    \item \textbf{Deep Learning}: Neural networks with sufficient data
    \item \textbf{Advanced Ensembles}: CatBoost, LightGBM integration
    \item \textbf{Spatial Models}: Geographically weighted regression
    \item \textbf{Time Series}: Market trend forecasting models
    \item \textbf{Explainability}: SHAP values for prediction interpretation
\end{enumerate}

\subsection{Production Features}

\begin{enumerate}
    \item \textbf{REST API}: Deploy as microservice with FastAPI
    \item \textbf{Model Monitoring}: Track prediction drift and retrain triggers
    \item \textbf{A/B Testing}: Compare model versions in production
    \item \textbf{Confidence Intervals}: Prediction uncertainty quantification
    \item \textbf{Batch Processing}: Efficient bulk price estimation
\end{enumerate}

%==============================================================================
\section{Conclusion}
%==============================================================================

\subsection{Summary of Achievements}

This project successfully developed \textbf{production-ready machine learning models} for apartment price prediction in Grand Tunis with the following accomplishments:

\begin{enumerate}
    \item \textbf{Exceeded Performance Goals}:
    \begin{itemize}
        \item Achieved R² $> 0.80$ on both v1 (raw) and v2 (cleaned) data
        \item MAE $\sim$38,000-40,000 TND (7-8\% of average price)
        \item Top-tier performance comparable to commercial systems
    \end{itemize}
    
    \item \textbf{Innovative Techniques}:
    \begin{itemize}
        \item KNN-based region imputation improving accuracy by 3-5\%
        \item City-specific outlier removal respecting local markets
        \item Elite ensemble strategy prioritizing stability over complexity
    \end{itemize}
    
    \item \textbf{Rigorous Methodology}:
    \begin{itemize}
        \item Systematic hyperparameter optimization
        \item Cross-validation ensuring generalization
        \item Bias-variance analysis preventing overfitting
        \item Parallel pipeline validation (v1 vs v2)
    \end{itemize}
    
    \item \textbf{Production Readiness}:
    \begin{itemize}
        \item Complete deployment package with inference scripts
        \item Standalone prediction capabilities
        \item Comprehensive documentation and metadata
    \end{itemize}
\end{enumerate}

\subsection{Impact \& Applications}

The developed models can support:

\begin{itemize}
    \item \textbf{Real Estate Agencies}: Automated property valuation tools
    \item \textbf{Buyers \& Sellers}: Fair price estimation and negotiation guidance
    \item \textbf{Financial Institutions}: Mortgage risk assessment
    \item \textbf{Urban Planners}: Market analysis and neighborhood development insights
    \item \textbf{Investors}: Portfolio valuation and opportunity identification
\end{itemize}

\subsection{Contribution to Course Objectives}

This project demonstrated mastery of core DMML concepts:

\begin{itemize}
    \item \textbf{Data Preprocessing}: Cleaning, outlier removal, imputation, encoding
    \item \textbf{Feature Engineering}: Derived features, domain knowledge application
    \item \textbf{Model Selection}: Systematic evaluation of multiple algorithms
    \item \textbf{Hyperparameter Tuning}: Grid search, random search, cross-validation
    \item \textbf{Ensemble Methods}: Voting, weighting, stacking strategies
    \item \textbf{Model Evaluation}: Multiple metrics, bias-variance analysis
    \item \textbf{Production Deployment}: Pipeline export, inference scripts
\end{itemize}

\subsection{Final Remarks}

With limited features (size, rooms, bathrooms, location), we achieved \textbf{80\% variance explanation} near-optimal given data constraints. Further improvements require additional data sources (building characteristics, external factors, geolocation), not just better algorithms.

This project proves that \textbf{thoughtful feature engineering and ensemble methods} can extract maximum predictive power from available data, creating practical tools for real-world applications.

\end{document}